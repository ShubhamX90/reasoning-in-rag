# QLoRA SFT config for Qwen2.5-7B-Instruct
base_model: "${PRJ}/models/Qwen2.5-7B-Instruct"     # set PRJ before running (we did in Step 2)
train_path: "${QWEN}/data/processed/sft/train.jsonl"
dev_path:   "${QWEN}/data/processed/sft/dev.jsonl"
output_dir: "${QWEN}/checkpoints/sft_qlora_v1"

# Sequence / packing
max_seq_len: 4096
packing: true

# LoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Optim & schedule
epochs: 6
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_ratio: 0.03
lr_scheduler_type: cosine

# BitsAndBytes (QLoRA)
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

# Training niceties
gradient_checkpointing: true
attn_implementation: sdpa   # safe default on your box
logging_steps: 20
eval_steps: 200
save_steps: 1000
save_total_limit: 3
seed: 42
